# -*- coding: utf-8 -*-
"""Copy of CIS_545_Fall_2022_HW_2_StudentVer_RK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yvuLq6_TavZ4AQzw9QJOhlH7C60ZuxI-

# CIS 5450 Homework 2: SQL
## Due: Thursday, October 13rd 2022, 10pm EST 
### Worth 100 points in total

Welcome to Homework 2! By now, you should be familiar with the world of data science and the Pandas library. This assignment focuses on helping you get to grips with a new tool: SQL.

Through this homework, we will be working with SQL (specifically **pandasql**) by exploring a Spotify dataset containing song reviews and statistics. We will also conduct some text analysis of song reviews.

 <!-- We will finish off the homework with some text analysis. -->

We are introducing a lot of new things in this homework, and this is often where students start to get lost. Thus, we **strongly** encourage you to review the slides/material as you work through this assignment. 

**Before you begin:**
- Be sure to click "Copy to Drive" to make sure you're working on your own personal version of the homework
- Check the pinned FAQ post on Ed for updates! If you have been stuck, chances are other students have also faced similar problems.

## Part 0: Libraries and Set Up Jargon (The usual wall of imports)
"""

!pip3 install penngrader
!pip install pandasql

from penngrader.grader import *
import pandas as pd
import datetime as dt
import pandasql as ps #SQL on Pandas Dataframe
import nltk
nltk.download('punkt')

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Three datasets we're using
! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_features.csv
! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_songs.csv
! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_rankings.csv

print(pd.__version__ )

"""### PennGrader Setup"""

# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW 
# TO ASSIGN POINTS TO YOU IN OUR BACKEND
STUDENT_ID = 15599572 # YOUR PENN-ID GOES HERE AS AN INTEGER #

grader = PennGrader(homework_id = 'CIS_5450_Fall22_HW2', student_id = STUDENT_ID)

"""# Music for everyone!

<br>
<center><img src = "https://upload.wikimedia.org/wikipedia/commons/3/33/Spotify_logo13.png" width= "500" align ="center"/></center>
<br>

We'll be working with a dataset containing Top Tracks on Spotify in 2017, along with their reviews and rankings.

In this homework, we'll be exploring the following attributes of the song data:

*   Song-specific metadata (eg. artist name) 

*   The songs' rankings based on date and listener location

*   Acoustic / musical properties of the song, eg. danceability


We'll be loading this data into various dataframes and querying them. We will primarily be using PandaSQL for these tasks. However, for some of the initial questions, we'll ask you to perform the same operations in Pandas. The purpose of conducting the same tasks in both Pandas and PandaSQL is to help you become more familiar with the similarites and differences of these two tools.

## Part 1: Load & Process our Datasets [9 points total]

Before we get into the data, we first need to load and clean our datasets. 

# Metadata
You'll be working with three CSV files:
- `spotify_features.csv`
- `songs_features.csv`
- `spotify_rankings.csv`

The file `spotify_features.csv` contains musical / acoustic data for each song, obtained via the Spotify API.

The file `songs_features.csv` includes the names, artists, Spotify URLs of the Top Songs of 2017. 

Each row in the file `spotify_rankings.csv` contains a song's ranking on a specific day in a particular region, where regions are sorted alphabetically.

For example, the first 200 rows of `spotify_rankings.csv` represent the ranks of various songs on 1st of January in Argentina. The next 200 rows contain the  rankings of songs during the 2nd of January in Argentina. 

Note that the Spotify dataset may containing missing data, so don't rely on the interval between successive dates as 200 rows. 

Instead, it would be preferable to read all the data and then filter by region & date. This way, you can be sure that you have the required data.


**TODO**:
* Load `spotify_songs.csv` and save the data to a dataframe called `songs_df`.
* Load `spotify_rankings.csv` and save the data to a dataframe called `rankings_df`.
* Load `spotify_features.csv` and save the data to a dataframe called `features_df` without the index column being included.

**Note**: If you see a column called `Unnamed: 0` when loading the data,be sure to drop this column while loading the dataset. This is necessary for one to receive full credit in all the following parts of this homework.
"""

# TODO: Import the datasets to pandas dataframes -- make sure the dataframes are named correctly! 
songs_df = pd.read_csv('spotify_songs.csv')
rankings_df = pd.read_csv('spotify_rankings.csv')
features_df = pd.read_csv('spotify_features.csv')

# view songs_df to make sure the import was successful
songs_df

# view rankings_df to make sure the import was successful
rankings_df

# view features_df to make sure the import was successful
features_df

"""### 1.1 Data Preprocessing
We are now going to clean our dataframes `songs_df` and `rankings_df` by performing the following tasks:       
1) fixing columns        
2) changing datatypes             
3) handling nulls.

First, let us view the first few rows of `songs_df`. You may also call `.info()` and check the cardinality of each column to view the specifics of the dataframe. This is a good first step to take during Exploratory Data Analysis (EDA).
"""

# view info information regarding movies_df
songs_df.info()

"""#### 1.1.1 Cleaning `songs_df`

`.info()` gives us meaningful information regarding columns, their types, and the amount of nulls. With this information, we can now clean our dataframe. 

**TODO**:
* Drop the column `time_signature`.
"""

# TODO: clean songs_df
songs_df = songs_df.drop(['time_signature'], axis=1)

# 3 points
grader.grade(test_case_id = 'test_cleaning_songs', answer = songs_df.head())

"""#### 1.1.2 Processing Rankings

`rankings_df` contains a `URL` column that represents the URL of the Spotify track. This URL contains the track ID. We would like to extract the ID corresponding to each song. 

To maintain consistency, begin by sorting this dataframe according to `Date` in ascending order. 

**TODO**:
- Sort the `Date` column in ascending order
- Split the strings in the `URL` column to extract the ID for each song, and save this as a new column called `ID` in `rankings_df`
- Drop the `URL` column

**Example**:
>URL | ID
>--- | ---
> https://open.spotify.com/track/3AEZUABDXNtecAOSC1qTf | 3AEZUABDXNtecAOSC1qTf

After performing these steps, `rankings_df` should have the following schema:
**Final Schema**:
>Position | Track Name | Artist | Streams | Date | Region | ID
>--- | --- | --- |--- |--- |--- |--- |

**Hint**: Note the delimiter within the URL and consult the documentation for the `.split()` function
"""

# TODO: extract ID and drop URL
rankings_df = rankings_df.sort_values('Date')
ID = rankings_df["URL"].str.split("/", n = 4, expand = True)
rankings_df['ID'] = ID[4]
rankings_df = rankings_df.drop(['URL', 'Unnamed: 0'], axis=1)

# 4 points
grader.grade(test_case_id = 'test_rankings_processing', answer = rankings_df.head())

"""#### 1.1.3 Cleaning `rankings_df`

Now let's clean `rankings_df` and make it usable.
"""

# Examine rankings_df using the .info() function
rankings_df.info()

"""
**TODO**:
* Drop all rows that have a null value in `rankings_df`
* Convert column `Date` into type `datetime64[ns]`.
"""

#TODO: 
rankings_df = rankings_df.dropna()
rankings_df['Date'] =  pd.to_datetime(rankings_df['Date'])

# 2 points
grader.grade(test_case_id = 'test_cleaning_rankings', answer = [len(rankings_df),rankings_df.head()])

"""### 1.2 Your Sandbox 

.info() is just one of many basic tools that you can use for Exploratory Data Analysis (EDA). Instead of throwing you straight into the deep end, we wanted to give you a chance to take some time and explore the data on your own. **This section is not graded**, so for the speedrunners out there feel free to just skip this section (1.2), but we wanted to at least give you a small space to utilize your EDA toolkit to familiarize yourself with all the data you just downloaded.

Some suggestions to get you started:
- `df.head()`
- `df.describe()`
- `Series.unique()`
"""

# Your EDA here! Feel free to add more cells54

"""## Part 2: Exploring the Data with PandasSQL (and Pandas) [62 points total]

Now that you're more familiar with the dataset, we'll now introduce you to SQL language. Specifically, we'll be using **pandasql**: a library that allows users to query Pandas DataFrames using SQL statements.

The typical flow of using pandasql (shortened to **ps**) is as follows:
1. Write a SQL query in the form of a string (Tip: use triple quotes '''x''' to write multi-line strings)
2. Run the query using **ps.sqldf(your_query, locals())**

PandaSQL is convenient as it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the dataframes `songs_df`, `rankings_df` and `features_df` that you have created above!

Given that SQL is a brand new language, we wanted to give you a chance to directly compare the similarities &differences of Pandas and SQL. 
Thus, for each of the simpler queries, we ask that you **perform the tasks in each question twice: once with Pandas and once with PandaSQL**. 

Each answer (unless specified otherwise) will thus require two dataframes, one dataframe whose name is prefixed with `pd_`, and another dataframe whose name is prefixed with `sql_`. You will submit these two dataframes seperately to the autograder. **We will be reviewing your code to make sure your code for each dataframe is written in the correct language.**

###2.1 Bruno Mars songs

#### 2.1.1 How many Bruno Mars' songs were popular in 2017?

Note: Only Pandas is required for this question.
The dataframe `songs_df` contains all top songs in 2017. We want to know whether Bruno Mars was a part of it (he obviously was) - but which of his songs made it to the top?

**TODO:** Using **pandas**, filter out the songs and reviews from `songs_df` that were by `Bruno Mars`. Then, save this data to a DataFrame called `bruno_df` that has the following schema:

>name | reviews
>--- | ---
"""

# TODO: Use pandas to obtain songs by `Bruno Mars`
bruno_df = songs_df[songs_df['artists'] == 'Bruno Mars']
bruno_df = bruno_df[['name', 'reviews']]

# 2 points
grader.grade(test_case_id = 'test_bruno', answer = bruno_df)

"""#### 2.1.2 How many of Bruno Mars' songs were deemed "good"?
We now want to see which of these songs contained the word "good" in the `reviews` column.

**TODO:** Using **pandasql**, update `bruno_df` so that it only contains songs that have the word 'good' in the `reviews` column.
"""

# TODO: Use pandasql to obtain only "good" songs of bruno mars
good_song_query = """ SELECT * FROM bruno_df WHERE reviews LIKE '%good%' """
bruno_df = ps.sqldf(good_song_query, locals())
bruno_df

# 2 points
grader.grade(test_case_id = 'test_good_songs_df', answer = bruno_df)

"""###2.2 Finding the hit songs

#### 2.2.1 Extract the total no. of streams
We now want to see what songs formed the top 75% of the year 2017 from `rankings_df`. We can measure the popularity of the songs using the total number of streams the song received.

**TODO**: 
* Find the total number of streams per song and save it into a dataframe called `streams_df`
* Understand the quartile ranges in `streams_df`

**Hint**: you may find it helpful to look up `.describe()` to understand quartiles. It would be helpful to save the necessary quartile value to use in the querying section that follows.
"""

# TODO: Using pandas extract the total number of streams per song from rankings_df
streams_df = rankings_df.groupby(by = 'Track Name').sum().reset_index().drop(['Position'], axis = 1)
streams_df.describe()

# 2 points
grader.grade(test_case_id = 'test_pd_describe', answer = len(streams_df))

"""#### 2.2.2 Top 75% of streams

Now that we've seen the distribution of the streams, we'd like to extract songs with streams within the top 75%, i.e 75% or more streams. For this part, use the quartile values you got via the `.describe()` function.

**TODO**: Using **pandas**, 
Filter out songs from `streams_df` whose stream count is in the top 75%, then save this data as the Pandas dataframe `pd_top_streams`.
"""

# TODO: Using pandas extract the top 75% based on number of streams

top75 = 2.133400e+04
pd_top_streams = streams_df[streams_df['Streams'] >= top75]
pd_top_streams

# 2 points
grader.grade(test_case_id = 'test_pd_songs', answer = pd_top_streams)

"""Repeat this process using **pandasql**, and save the data in the variable `sql_top_streams`"""

# TODO: Using pandasql extract the top 75% based on number of streams
top_query = """
  SELECT * 
  FROM streams_df
  WHERE Streams >= 2.133400e+04
"""

sql_top_streams = ps.sqldf(top_query, locals())
sql_top_streams

# 2 points
grader.grade(test_case_id = 'test_sql_songs', answer = sql_top_streams)

"""### 2.3 Duration of songs

Now that we know which songs are hits, we'd like to listen to songs that are not too short nor too long.

**TODO**: Using **pandas**, 
Filter out songs from `songs_df` whose duration is between 3 and 5 minutes.

- Create a new column in `songs_df` called "duration_min" that converts the duration in "duration_ms" from milliseconds to minutes
- Extract only songs whose duration is at least 3 minutes and at most 5 minutes. Then, save the output to `ideal_songs_df`.
"""

songs_df['duration_min'] = songs_df['duration_ms']/60000
ideal_songs_df = songs_df[songs_df['duration_min'].between(3, 5)]
ideal_songs_df

# 2 points
grader.grade(test_case_id = 'test_song_duration', answer = ideal_songs_df)

"""### 2.4 Who are the highest ranked artists?

Which artists have been ranked #1 the most times in 2017?

**TODO**: Using the dataframe `rankings_df`, perform the following tasks twice, once using **pandas** and once using **pandasql**:
- Extract the names of artist that have `position` as 1, and store this data in `pd_pos_df`
- Using `pd_pos_df`, find the number of times each artist was ranked #1
- Get the Top 10 artists, i.e. the 10 artists which have been ranked #1 the most times. 

The dataframe `pd_pos_df` should have the following schema:

>Artist | Position
>--- | ---
"""

rankings_df

# TODO: pandas version
pd_pos_df = rankings_df[rankings_df['Position'] == 1]
pd_pos_df = pd_pos_df.groupby('Artist').count().reset_index()
pd_pos_df = pd_pos_df[['Artist', 'Position']]
pd_pos_df = pd_pos_df.sort_values(by = ['Position','Artist'], ascending = [False, True]).iloc[0:10]
pd_pos_df

# TODO: pandasql version
pos_query = """ 
  SELECT Artist, COUNT(Position) as Position
  FROM rankings_df
  WHERE Position = 1 
  GROUP BY Artist
  ORDER BY COUNT(Position) DESC
  LIMIT 10
"""

sql_pos_df = ps.sqldf(pos_query, locals())
sql_pos_df

# 6 points
grader.grade(test_case_id = 'test_artist_rank', answer = (pd_pos_df, sql_pos_df, pos_query))

"""### 2.5 Popular Artists!

Are there artists whose songs are streamed more often than others? Let's find out!

**TODO:**

Perform the following task involving the dataframe `rankings_df` twice, once using **pandas** and once using **pandasql**. Call the output dataframe `pd_summer_df` or `sql_summer_df` (based on whether you used pandas or pandasql).

- Consider rows in `rankings_df` that are during Summer 2017 
  - Note: Consider the duration of summer to be from 15th June 2017 to 16th September 2017 (both dates inclusive)
- Find the total number of streams corresponding to each artist, then store this data in a new column called `Number`. 
- Sort this dataframe on the `Number` column so that the most popular artists appear first (i.e. sort according to the `Number` column in descending order).
- Remember to call the output dataframe `pd_summer_df` or `sql_summer_df` (based on whether you used pandas or pandasql).

The dataframes `pd_summer_df` and `sql_summer_df` should both have the following schema:

>Artist | Number
>--- | ---
"""

rankings_df

# TODO: pandas
pd_summer_df = rankings_df[rankings_df['Date'].between(pd.datetime(2017, 6, 15), pd.datetime(2017, 9, 16))]
pd_summer_df = pd_summer_df.groupby(['Artist']).sum().reset_index().drop(['Position'], axis = 1).sort_values(by = 'Streams', ascending = False)
pd_summer_df = pd_summer_df.rename(columns = {'Streams': 'Number'})
pd_summer_df

#TODO: pandasql
summer_query = """
  SELECT Artist, SUM(Streams) as Number
  FROM rankings_df
  WHERE strftime('%Y-%m-%d',Date) 
  BETWEEN '2017-06-15'
  AND  '2017-09-16'
  GROUP BY Artist
  ORDER BY Number DESC
"""

sql_summer_df = ps.sqldf(summer_query, locals())
sql_summer_df

# 7 points
grader.grade(test_case_id = 'test_summer', answer = (summer_query,pd_summer_df,sql_summer_df))

"""### 2.6 Which songs are danceable but also mellow?

Now let us switch gears and examine `songs_df` and `features_df`. In particular, we want to find the songs with high danceability and low tempo.

**TODO**: Perform the following tasks, once using **pandas** and once using **pandasql**:
- Before performing any operations,  round the `danceability` column to one decimal place, and call the resultant column `r_danceability`. This will allow us to conduct a more general (coarser) analysis of the data.
- Merge `songs_df` and `features_df`, then sort the songs with danceability in *descending* order and tempo in *ascending* order. (When sorting, make sure to use the `r_danceability` column.)
- Call the output dataframe `pd_songs_features_df` or `sql_songs_features_df` based on whether you used pandas or pandasql.
"""

features_df['r_danceability'] = features_df['danceability'].round(1)
features_df

songs_df

# TODO: pandas
pd_songs_features_df = songs_df.merge(features_df, on = 'id').sort_values(
    by = ['r_danceability', 'tempo'], ascending = [False, True])
pd_songs_features_df

# 4 points
grader.grade(test_case_id = 'test_danceability_tempo_pd', answer = pd_songs_features_df[['id', 'r_danceability', 'tempo']])

# TODO: pandasql version
song_feature_query = """
  SELECT  songs_df.id, 
          ROUND(features_df.danceability, 1) as r_danceability, 
          features_df.tempo,
          features_df.id
  FROM songs_df 
  LEFT JOIN features_df
  On songs_df.id = features_df.id
  ORDER BY r_danceability DESC, features_df.tempo ASC
"""


sql_song_features_df = ps.sqldf(song_feature_query, locals())
sql_song_features_df

# 4 points
grader.grade(test_case_id = 'test_danceability_tempo_sql', answer = sql_song_features_df)

"""### 2.7 Do we like the same songs?

#### 2.7.1 Which regions have the most streams?

**TODO**: 
Perform the following tasks, once using **pandas** and once using **pandasql**:
- Extract rows belonging to the top 2 regions that have the most streams. 
- Store your output in a new dataframe called `pd_top_regions_df` or `sql_top_regions_df` based on whether you used pandas or pandasql. Both of these dataframes should have same schema as `rankings_df`. Sort both these dataframes in ascending order by `Streams`.

Note: Since we want to focus on specific regions, we should disregard rows where the `Region` column has the value `"global"`.
"""

rankings_df

# TODO: pandas
temp = rankings_df.groupby('Region').sum().reset_index().sort_values(by = 'Streams', ascending = False)
pd_top_regions_df = rankings_df[rankings_df['Region'].isin(['global', 'us'])]
pd_top_regions_df

#TODO: pandasql
top_regions_query = """
  SELECT * 
  FROM rankings_df 
  WHERE Region IN (
    SELECT Region
    FROM rankings_df
    GROUP BY Region
    ORDER BY SUM(Streams) DESC
    LIMIT 2
  )
"""

sql_top_regions_df = ps.sqldf(top_regions_query, locals())
sql_top_regions_df

# 7 points
grader.grade(test_case_id = 'test_top_regions', answer = (top_regions_query,  pd_top_regions_df.head(1000),  sql_top_regions_df.head(1000), 
                                                          len(pd_top_regions_df), len(sql_top_regions_df)))

"""#### 2.7.2 Do the regions with the most streams like different songs?

**TODO**: Perform the following task, once using **pandas** and once using **pandasql**:
- Find the songs that the two regions (found in 2.7.1) **DO NOT** have in common
- Store the result in a new dataframe called `pd_diff_tracks_df` or `sql_diff_tracks_df` (based on whether you used pandas or pandasql). These two dataframes should both have the following schema:

>ID | Track Name | Artist
>--- | --- | ---

**Hint**: Do we need to join any tables here? If yes, which type of join should we use?
"""

pd_top_regions_df

# TODO: pandas
pd_global_songs_df = pd_top_regions_df[pd_top_regions_df['Region'] == 'global'].drop_duplicates(subset=['ID'])
pd_us_songs_df = pd_top_regions_df[pd_top_regions_df['Region'] == 'us'].drop_duplicates(subset=['ID'])
pd_same_songs_df = pd_global_songs_df.merge(pd_us_songs_df, on = 'ID', how = 'inner')
pd_diff_tracks_df = pd_top_regions_df[['ID', 'Track Name', 'Artist']].drop_duplicates(subset=['ID'])
# pd_diff_tracks_df = pd_diff_tracks_df[~pd_diff_tracks_df['ID'].isin(pd_same_songs_df['ID'])]
pd_diff_tracks_df

#TODO: pandasql
diff_tracks_query = """
  SELECT global.ID, global."Track Name", global.Artist
  FROM pd_global_songs_df global
       LEFT JOIN pd_us_songs_df us
          ON global.ID = us.ID

  UNION ALL

  SELECT us.ID, us."Track Name", us.Artist
  FROM pd_us_songs_df us
       LEFT JOIN pd_global_songs_df global
          ON global.ID = us.ID

  WHERE global.ID IS NULL
  OR us.ID is NULL
"""

sql_diff_tracks_df = ps.sqldf(diff_tracks_query, locals())
sql_diff_tracks_df

# 15 points
grader.grade(test_case_id = 'test_diff_tracks', answer = (diff_tracks_query, pd_diff_tracks_df, sql_diff_tracks_df))

"""### 2.8 New Years Eve Party!

Who doesn't love to dance? Let's find some songs to groove to!

**TODO**: Perform the following tasks using **pandasql** only:

- Find the songs that made it to the charts in December 2017 (2017-12-01 to 2017-12-31) and whose duration is longer than 3 minutes
- Find the artist and the danceability of these songs. Be sure to only include songs with danceability > 0.5.
- Store the result in a new dataframe called `sql_dance_df` that has the following schema:

>Artist | Track Name | danceability
>--- | --- | ---


**Hint**: Think about which data resides in which table!
"""

features_df

#TODO: pandasql
dance_query = """
  SELECT distinct name as "Track Name", artists as Artist, danceability
  FROM songs_df 
  LEFT JOIN rankings_df 
  ON songs_df.id = rankings_df.ID
  LEFT JOIN features_df 
  ON songs_df.id = features_df.id
  WHERE duration_ms > 180000
  AND Date BETWEEN '2017-12-01' AND '2017-12-31'
  AND danceability > 0.5

"""

sql_dance_df = ps.sqldf(dance_query, locals())
sql_dance_df

# 7 points
grader.grade(test_case_id = 'test_dance', answer = (dance_query, sql_dance_df))

"""## Part 3: Data Visualization [15 points total -- MANUALLY GRADED]

The popularity of songs fluctuates as time progresses. We want to create a graph that illustrates the no. of streams for the most and least popular songs during each month in 2017.

Perform the following tasks:
- Find the song that had the *most* streams on `2017-01-01`
- Find the song that had the *least* streams on `2017-01-01`
- Find the no. of streams that these two songs received on the first day of each month in 2017 (eg. 2017-01-01, 2017-02-01, 2017-03-01 ... 2017-12-01) 

Plot a line graph which shows the trend you found! Make sure you use the ID of the two songs when creating this graph.

**Hint**: This link is a useful resource: https://seaborn.pydata.org/generated/seaborn.lineplot.html

Make sure your line graph has the following features:
1. The X-axis should be labelled "Date", and the Y-axis should be labelled "Streams".
2. There should be markers on the plot to specify the no. of streams each song received ia particular month.
3. The lines corresponding to the two songs should have different colors.

This section will be **manually graded**.
"""

twosongs = rankings_df[rankings_df['Date'] == pd.datetime(2017, 1, 1)] 
most = twosongs.groupby('ID').sum().reset_index().sort_values(by = 'Streams', ascending = False).iloc[0:1]['ID']
print(most)
#5aAx2yezTd8zXrkmtKl66
least = twosongs.groupby('ID').sum().reset_index().sort_values(by = 'Streams', ascending = True).iloc[0:1]['ID']
print(least)
#7qxgfIAuWUY9VHLn35Sqw
most_df = rankings_df[rankings_df['ID'] == '5aAx2yezTd8zXrkmtKl66']
least_df = rankings_df[rankings_df['ID'] == '7qxgfIAuWUY9VHLn35Sqw']

most_streams = most_df.groupby('Date').sum().reset_index()
most_streams = most_streams[most_streams['Date'].isin(['2017-01-01', 
                                                       '2017-02-01', 
                                                       '2017-03-01', 
                                                       '2017-04-01', 
                                                       '2017-05-01', 
                                                       '2017-06-01', 
                                                       '2017-07-01', 
                                                       '2017-08-01', 
                                                       '2017-09-01', 
                                                       '2017-10-01', 
                                                       '2017-11-01', 
                                                       '2017-12-01'])]
                                                     
# most_streams = most_streams['Streams']
# dates = ['2017-01-01','2017-02-01', '2017-03-01', '2017-04-01', '2017-05-01', '2017-06-01', '2017-07-01', 
#           '2017-08-01', '2017-09-01', '2017-10-01', '2017-11-01', '2017-12-01']
most_streams = most_streams.rename(columns = {'Streams':'5aAx2yezTd8zXrkmtKl66'}).drop('Position', axis = 1)
most_streams

least_streams = least_df.groupby('Date').sum().reset_index()
least_streams = least_streams[least_streams['Date'].isin(['2017-01-01', 
                                                       '2017-02-01', 
                                                       '2017-03-01', 
                                                       '2017-04-01', 
                                                       '2017-05-01', 
                                                       '2017-06-01', 
                                                       '2017-07-01', 
                                                       '2017-08-01', 
                                                       '2017-09-01', 
                                                       '2017-10-01', 
                                                       '2017-11-01', 
                                                       '2017-12-01'])]
                                                     
least_streams = least_streams.rename(columns = {'Streams':'7qxgfIAuWUY9VHLn35Sqw'}).drop('Position', axis = 1)
least_streams

# Prepare a dataset for plot 
plot_df = most_streams.merge(least_streams, on = 'Date', how = 'left')
plot_df

# TODO: Plot a line chart
import seaborn as sns


sns.lineplot(x='Date', y='Streams', hue='Songs', size = 20, 
             data=pd.melt(plot_df, ['Date'], value_name='Streams', var_name='Songs'))
sns.set(rc={'figure.figsize':(30,20)})

for v in plot_df.iterrows():
  plt.text(v[1][0], v[1][1], f'{v[1][1]}')
for v in plot_df.iterrows():
  plt.text(v[1][0], v[1][2], f'{v[1][2]}')

"""## Part 4: Working with Text Data [14 points]

Now, let's switch gears and try to text-based analysis. Textual data is complex, but can also be used to generate extremely interpretable results, making it both valuable and interesting. 

Throughout this section, we will attempt to answer the following question:

**According to the `songs_df` dataframe, what do the reviews for the Top Tracks of 2017 look like?**

###4.1 Tokenizing the text

We are going to split the contents of in the Reviews column into a list of words. We will use the **nltk** library, which contains an extensive set of tools for text processing. Now, this homework would be interminably long if we went into all the details of nltk. Thus, we are only going to use the following components of the library:
- `nltk.word_tokenize()`: a function used to tokenize text
- `nltk.corpus.stopwords`: a list of commonly used words such as "a", "an","in" that are often ignored in text analysis

Note that for this question, we didn't have to clean the text data first as our original dataset was well-formatted. However, in practice, we would typically clean the text first using regular expressions (regex). Keep this in mind as you work on the project later on in the semester.

**TODO:** Perform the following tasks:
- Use **nltk.corpus.stopwords** to create a set containing the most common English stopwords.
- Implement the function **tokenized_content(content)**, which takes in a string and does the following:
1. Tokenize the text
2. Keep tokens that only contain alphabetic characters (i.e. tokens with no punctuation)
3. Convert each token to lowercase
4. Remove stopwords (commonly used words such as "a", "an", "in")
"""

import nltk
nltk.__version__

from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = set(stopwords.words('english'))

# TODO: tokenize and flatten
def tokenize_content(content):
  words = nltk.word_tokenize(content)
  words=[word.lower() for word in words if word.isalpha()]
  words = [w for w in words if w not in stopwords]

  return words

"""**TODO**: Also perform the following tasks: 
- Extract the `reviews` column of `songs_df` as a list called `reviews`. 
- Apply your `tokenize_content()` function to each item in the list `reviews`. Call the resultant list `top_tokens_list`. 
- Flatten the list `top_tokens_list`, and call the resultant list **top_tokens**. The autograder will be examining the contents of this list.
"""

# TODO: tokenize and flatten
reviews = songs_df['reviews']

top_tokens_list = [tokenize_content(word) for word in reviews]

top_tokens = []

for element in top_tokens_list:
        if type(element) is list:
            for item in element:
                top_tokens.append(item)
        else:
            top_tokens.append(element)

# 2 point
grader.grade(test_case_id = 'test_top_tokens', answer = len(top_tokens))

"""### 4.2 Most Frequent Words
**TODO**: Now, find the 20 most common words in the list `top_tokens`. Save the result as a list of `(word, count)` tuples, in descending order of `count`.

**Hint**: For this question, you can use `Counter` from the Python `collections` library: https://docs.python.org/2/library/collections.html#counter-objects
"""

from collections import Counter
top_most_common = Counter(top_tokens).most_common(20)
top_most_common

# 2 points
grader.grade(test_case_id = 'test_top_most_common', answer = top_most_common)

"""### 4.3 Word Clouds [10 points]

Before we move on from this dataset, let's visualize our results using a word cloud.

**TODO**: Create a word cloud containing all the words in the list `top_tokens` (created in question 4.1). [The WordCloud documentation](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) contains instructions on how to do this. 

*Please make sure your wordcloud has a **white** background.*

We will be going through your notebooks and **manually grading** your word cloud. This is worth 10 points. 
"""

# TODO: make a word cloud for top tokens (MANUALLY GRADED)
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd

text = " ".join(title for title in top_tokens)

word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)

plt.figure(figsize = (15, 15), facecolor = None)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")

plt.show()

"""# HW Submission

<br>
<center><img src = "https://memegenerator.net/img/instances/73124265/good-job.jpg" width= "500" align ="center"/></center>
<br>

Congratulations on finishing this homework! The good news is that similar to HW1, you basically know your score when you submit to Gradescope. 
However, this time, we will be manually grading your lineplots and wordclouds, so the autograder score is not final! Remember that we will also be checking for plagiarism, so please make sure to cite your sources (if any) by commenting the urls / links you looked at.

Before you submit on Gradescope (you must submit your notebook to receive credit):

1.   Please rerun your notebook on Colab by clicking "Restart and Run-All", and make sure there is nothing wrong with your notebook.
2.   **Double check that you have the correct PennID (all numbers) saved in the autograder**. 
3. Make sure you've run all the PennGrader cells and have received a score.
4. Go to the "File" tab at the top left, and click "Download .ipynb" + "Download .py". Please name the `.ipynb` and `.py` files **"homework2.ipynb"** and **"homework2.py"** respectively. Then, upload both the `.py` and `.ipynb` files to Gradescope. 

###Be sure to name your files correctly!!!

**Please let the course staff know ASAP if you have any issues submitting.**
"""